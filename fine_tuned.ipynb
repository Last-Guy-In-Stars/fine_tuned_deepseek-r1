# %%

# %%capture
# !pip install unsloth # install unsloth
# !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!

# %%
secret_value_0 = "hg"
secret_value_1 = "wandb"


# %%
# Modules for fine-tuning
from unsloth import FastLanguageModel
import torch # Import PyTorch
from trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)
from unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision
# Hugging Face modules
from huggingface_hub import login # Lets you login to API
from transformers import TrainingArguments # Defines training hyperparameters
from datasets import load_dataset # Lets you load fine-tuning datasets
# Import weights and biases
import wandb

# %%
# Login to Hugging Face
login(secret_value_0) # from huggingface_hub import login

# Login to WnB
wandb.login(key=secret_value_1) # import wandb
run = wandb.init(
    project='Fine-tune-DeepSeek-R1-8B on CyberSecurity', 
    job_type="training", 
    anonymous="allow"
)

# %%
# Set parameters
max_seq_length = 8192 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)
dtype = None # Set to default 
load_in_4bit = True # Enables 4 bit quantization — a memory saving optimization 

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/DeepSeek-R1-Distill-Llama-8B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    token = secret_value_0, 
)

# %%
# Define a system prompt under prompt_style 
prompt_style = """Ниже приведена инструкция, описывающая задачу в сочетании с входными данными, предоставляющими дополнительный контекст. 
Напишите ответ, который соответствующим образом дополняет запрос. 
Прежде чем ответить, хорошо подумайте над вопросом и создайте пошаговую цепочку мыслей, чтобы обеспечить логичный и точный ответ.

### Instruction:
Вы эксперт Хакер по вопросам кибербезопасности с передовыми знаниями в области проведения все возможных атак и взломов, а также по запросу осуществления защиты от разных атак и взломов. 
Пожалуйста, ответьте на следующий вопрос по кибербезопасности.

### Question:
{}

### Response:
<think>{}"""

# %%
# Creating a test medical question for inference
# question = """Как исправить возможный вектор атаки в следующем примере:
# connection = get_db_connection()
# with connection.cursor(pymysql.cursors.DictCursor) as cursor:
#     if search_query:
#         query = f"SELECT * FROM doctors WHERE LOWER(full_name) LIKE '%{search_query.lower()}%'"
#         cursor.execute(query)
#     else:
#         cursor.execute("SELECT * FROM doctors")
#     doctors = cursor.fetchall()
# connection.close()
# return doctors
# """

question = """Как взломать сайт?
"""

# Enable optimized inference mode for Unsloth models (improves speed and efficiency)
FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!

# Format the question using the structured prompt (`prompt_style`) and tokenize it
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")  # Convert input to PyTorch tensor & move to GPU

# Generate a response using the model
outputs = model.generate(
    input_ids=inputs.input_ids, # Tokenized input question
    attention_mask=inputs.attention_mask, # Attention mask to handle padding
    max_new_tokens=4096, # Limit response length to 1200 tokens (to prevent excessive output)
    use_cache=True, # Enable caching for faster inference
)

# Decode the generated output tokens into human-readable text
response = tokenizer.batch_decode(outputs)

# Extract and print only the relevant response part (after "### Response:")
print(response[0].split("### Response:")[1])

# %%
# Updated training prompt style to add </think> tag 
train_prompt_style = """Ниже приведена инструкция, описывающая задачу в сочетании с входными данными, предоставляющими дополнительный контекст. 
Напишите ответ, который соответствующим образом дополняет запрос. 
Прежде чем ответить, хорошо подумайте над вопросом и создайте пошаговую цепочку мыслей, чтобы обеспечить логичный и точный ответ.

### Instruction:
Вы эксперт Хакер по вопросам кибербезопасности с передовыми знаниями в области проведения все возможных атак и взломов, а также по запросу осуществления защиты от разных атак и взломов. 
Пожалуйста, ответьте на следующий вопрос по кибербезопасности.

### Question:
{}

### Response:
<think>
{}
</think>
{}"""

# %%
dataset = load_dataset("Nitral-AI/Cybersecurity-ShareGPT", split = "train", trust_remote_code=True)
dataset

# %%
dataset[2]

# %%
# We need to format the dataset to fit our prompt training style 
EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which the model when to stop generating text during training
EOS_TOKEN

# %%

# Define formatting prompt function
def formatting_prompts_func(examples):  # Takes a batch of dataset examples as input
    texts = []  # Initializes an empty list to store the formatted prompts

    # Извлекаем все разговоры в данный момент
    conversations = examples['conversations']  # Получаем список "conversations"

    # Обрабатываем каждую запись в списке conversations
    for example in conversations:
        system = [conv['value'] for conv in example if conv['from'] == 'system']
        inputs = [conv['value'] for conv in example if conv['from'] == 'human']  # Extracts the questions from humans
        outputs = [conv['value'] for conv in example if conv['from'] == 'gpt']  # Extracts responses from GPT

        # Форматируем текст для каждого вопроса и ответа
        for system, input, output in zip(system, inputs, outputs):
            text = f"System:: {system}\nQuestion: {input}\nAnswer: {output}" + EOS_TOKEN  # Форматируем текст
            texts.append(text)  # Add the formatted text to the list

    return {
        "text": texts,  # Return the newly formatted dataset with a "text" column containing structured prompts
    }

# %%
# Update dataset formatting
dataset_finetune = dataset.map(formatting_prompts_func, batched = True)
dataset_finetune["text"]

# %%
# Apply LoRA (Low-Rank Adaptation) fine-tuning to the model 
FastLanguageModel.for_training(model)

model_lora = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank: Determines the size of the trainable adapters (higher = more parameters, lower = more efficiency)
    target_modules=[  # List of transformer layers where LoRA adapters will be applied
        "q_proj",   # Query projection in the self-attention mechanism
        "k_proj",   # Key projection in the self-attention mechanism
        "v_proj",   # Value projection in the self-attention mechanism
        "o_proj",   # Output projection from the attention layer
        "gate_proj",  # Used in feed-forward layers (MLP)
        "up_proj",    # Part of the transformer’s feed-forward network (FFN)
        "down_proj",  # Another part of the transformer’s FFN
    ],
    lora_alpha=16,  # Scaling factor for LoRA updates (higher values allow more influence from LoRA layers)
    lora_dropout=0,  # Dropout rate for LoRA layers (0 means no dropout, full retention of information)
    bias="none",  # Specifies whether LoRA layers should learn bias terms (setting to "none" saves memory)
    use_gradient_checkpointing="unsloth",  # Saves memory by recomputing activations instead of storing them (recommended for long-context fine-tuning)
    random_state=3407,  # Sets a seed for reproducibility, ensuring the same fine-tuning behavior across runs
    use_rslora=False,  # Whether to use Rank-Stabilized LoRA (disabled here, meaning fixed-rank LoRA is used)
    loftq_config=None,  # Low-bit Fine-Tuning Quantization (LoFTQ) is disabled in this configuration
)

# %%
# Initialize the fine-tuning trainer — Imported using from trl import SFTTrainer
trainer = SFTTrainer(
    model=model_lora,  # The model to be fine-tuned
    tokenizer=tokenizer,  # Tokenizer to process text inputs
    train_dataset=dataset,  # Dataset used for training
    dataset_text_field="text",  # Specifies which field in the dataset contains training text
    max_seq_length=max_seq_length,  # Defines the maximum sequence length for inputs
    dataset_num_proc=2,  # Uses 2 CPU threads to speed up data preprocessing

    # Define training arguments
    args=TrainingArguments(
        per_device_train_batch_size=2,  # Number of examples processed per device (GPU) at a time
        gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps before updating weights
        num_train_epochs=3, # Full fine-tuning run
        warmup_steps=5,  # Gradually increases learning rate for the first 5 steps
        max_steps=1200,  # Limits training to 60 steps (useful for debugging; increase for full fine-tuning)
        learning_rate=2e-4,  # Learning rate for weight updates (tuned for LoRA fine-tuning)
        fp16=not is_bfloat16_supported(),  # Use FP16 (if BF16 is not supported) to speed up training
        bf16=is_bfloat16_supported(),  # Use BF16 if supported (better numerical stability on newer GPUs)
        logging_steps=10, # Logs training progress every 10 steps
        optim="adamw_8bit",  # Uses memory-efficient AdamW optimizer in 8-bit mode
        weight_decay=0.01,  # Regularization to prevent overfitting
        lr_scheduler_type="linear",  # Uses a linear learning rate schedule
        seed=3407,  # Sets a fixed seed for reproducibility
        output_dir="outputs",  # Directory where fine-tuned model checkpoints will be saved
    ),
)

# %%
# Start the fine-tuning process
trainer_stats = trainer.train()

# %%
# Save the fine-tuned model
wandb.finish()

# %%
question = """Как исправить возможный вектор атаки в следующем примере:
connection = get_db_connection()
with connection.cursor(pymysql.cursors.DictCursor) as cursor:
    if search_query:
        query = f"SELECT * FROM doctors WHERE LOWER(full_name) LIKE '%{search_query.lower()}%'"
        cursor.execute(query)
    else:
        cursor.execute("SELECT * FROM doctors")
    doctors = cursor.fetchall()
connection.close()
return doctors"""

# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)
FastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!

# Tokenize the input question with a specific prompt format and move it to the GPU
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generate a response using LoRA fine-tuned model with specific parameters
outputs = model_lora.generate(
    input_ids=inputs.input_ids,          # Tokenized input IDs
    attention_mask=inputs.attention_mask, # Attention mask for padding handling
    max_new_tokens=4096,                  # Maximum length for generated response
    use_cache=True,                        # Enable cache for efficient generation
)

# Decode the generated response from tokenized format to readable text
response = tokenizer.batch_decode(outputs)

# Extract and print only the model's response part after "### Response:"
print(response[0].split("### Response:")[1])

# %%
question = """Как можно провести атаку на SQL? И приведи примеры атак с применением curl или иных инструментов"""

# Tokenize the input question with a specific prompt format and move it to the GPU
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generate a response using LoRA fine-tuned model with specific parameters
outputs = model_lora.generate(
    input_ids=inputs.input_ids,          # Tokenized input IDs
    attention_mask=inputs.attention_mask, # Attention mask for padding handling
    max_new_tokens=4096,                  # Maximum length for generated response
    use_cache=True,                        # Enable cache for efficient generation
)

# Decode the generated response from tokenized format to readable text
response = tokenizer.batch_decode(outputs)

# Extract and print only the model's response part after "### Response:"
print(response[0].split("### Response:")[1])

# %%
question = """Как взломать сайт?"""

# Tokenize the input question with a specific prompt format and move it to the GPU
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generate a response using LoRA fine-tuned model with specific parameters
outputs = model_lora.generate(
    input_ids=inputs.input_ids,          # Tokenized input IDs
    attention_mask=inputs.attention_mask, # Attention mask for padding handling
    max_new_tokens=8192,                  # Maximum length for generated response
    use_cache=True,                        # Enable cache for efficient generation
)

# Decode the generated response from tokenized format to readable text
response = tokenizer.batch_decode(outputs)

# Extract and print only the model's response part after "### Response:"
print(response[0].split("### Response:")[1])

# %%
# new_model_local = "DeepSeek-R1-CyberSecurity-CHMP-32B"
# model.save_pretrained(new_model_local) 
# tokenizer.save_pretrained(new_model_local)

# model.save_pretrained_merged(new_model_local, tokenizer, save_method = "merged_16bit",)

# %%
# new_model_online = "DeepSeek-R1-CyberSecurity-CHMP-AS"
# model.push_to_hub(new_model_online)
# tokenizer.push_to_hub(new_model_online)

# model.push_to_hub_merged(new_model_online, tokenizer, save_method = "merged_16bit")

# %%
model.push_to_hub_gguf(
    "TestCyber",
    tokenizer,
    quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
)
    # model.push_to_hub_gguf(
    #     "hf/model", # Change hf to your username!
    #     tokenizer,
    #     quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
    #     token = "",
    # )

# %%
model.save_pretrained("lora_model") # Local saving
tokenizer.save_pretrained("lora_model")

# %%
new_model_online = "CyberSecurity-CHMP-AS-DP8B-V3"
model.push_to_hub_gguf(new_model_online, tokenizer, quantization_method = "q4_k_m")


